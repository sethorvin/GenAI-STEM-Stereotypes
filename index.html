<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>STEM Stereotypes and Generative AI</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="icon" href ="images/logoAI.png" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>

	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header" class="alt">
						<!--<span class="logo"><img src="images/logoAI.svg" alt="" /></span> -->
						<img src="images/logoAI.svg" alt="" class="icon-custom" >
						<h1>STEM Stereotypes Through the Lens of Generative Artificial Intelligence</h1>
						<p>A project investigating the influence of societal biases of race and gender on the output of text-to-image AI tools.<br />
						</p>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul>
							<li><a href="#review">About Stereotypes & AI</a></li>
							<li><a href="#methods" class="active">Methods</a></li>
							<li><a href="#imagefx">ImageFX from Google</a></li>
							<li><a href="#stablediffusion">Stable Diffusion</a></li>
							<li><a href="#adobefirefly">Adobe Firefly</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Review -->
						<section id="review" class="main">
							<div class="spotlight">
								<div class="content">
									<header class="major">
										<h2>About Stereotypes & AI</h2>
									</header>
									<p>Generative artificial intelligence has been found to produce output that often aligns with societal biases of race and gender. When prompted to generate images of surgeons, two leading text-to-image models “[depicted] over 98% of surgeons as white and male." Similarly, the textual classification of 10,000 portrait photographs through a Contrastive Language-Image Pretraining model created “a strong positive correlation … between labels Female and Attractive, Male and Rich, as well as White Person and Attractive”. And a study by Bloomberg “generated thousands of images related to job titles and crime” using the AI tool Stable Diffusion and found that images generated for high-paying occupations disproportionately featured men with lighter skin tones while those for low-paying occupations often featured women with darker skin tones. The images generated for criminals and terrorists disproportionately included men with darker skin tones, many of whom were rendered as stereotypes of Muslim men. 
										
									</p>
									<p>STEM fields are notorious for gender and racial stereotypes. Such biases are especially common within computing and engineering fields, in which even children as young as six believe that girls are fundamentally less interested in computer science and engineering. Throughout STEM, the underrepresentation of Black and Latinx people is often “falsely attributed to personal characteristics such as “inferior intelligence [and] weak work ethic” while the representation of Asian students “is explained by such stereotypes as superior intelligence, strong work ethic, or excelling in math, all of which are a part of the model minority concept”.5 Many occupation-specific stereotypes also exist, such as the notion that “if you aren’t white, and you aren’t Asian, and you aren’t Indian, you aren’t an engineer.”
									</p>
								</div>
								<span class="image"><img src="images/pic01.jpg" alt="" /></span>
							</div>
						</section>

						<!-- Methods -->
							<section id="methods" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Methods</h2>
										</header>
										<p>Three text-to-image generative AI tools were selected for study: ImageFX from Google, Stable Diffusion, and Adobe Firefly. These models were selected for their realism and free accessibility through either a web browser or a Python API. Ten STEM occupations representing a range of average incomes were selected: software developer, statistician, civil engineer, physicist, architect, chemist, geoscientist, environmental scientist, nutritionist, and biologist. For each occupation, each AI tool was used to generate ten artificial images of a worker using the prompt “A color photograph of a ___, centered headshot, high-quality”, generating 300 images in total. Each image was required to be a professional headshot of a single worker with minimal content outside of the subject; images that deviated from this requirement were rejected. 
										</p>
										<p>After the images were collected, the subject of each image was manually labeled as either a “man” or a “woman”, and the rates of women generated per occupation were calculated for each AI tool. Using OpenCV (an open-source machine learning software library for image analysis), a Python program was implemented that (for each image) automatically crops the image around the subject’s face, identifies which pixels are skin, and determines the color that appears most frequently throughout all skin pixels. For each subject, this color was selected as their typical skin color. Some images were inadequately cropped by OpenCV to include parts of the background or hair, resulting in inaccurate color selection for skin tone. In such cases, affected images were manually cropped prior to skin tone analysis.
										</p>
									</div>
									<span class="image"><img src="images/pic01.jpg" alt="" /></span>
								</div>
							</section>

						<!-- ImageFX Section 
							<section id="imagefx" class="main special">
								<header class="major">
									<h2>ImageFX from Google</h2>
								</header>
								<ul class="features">
									<li>
										<span class="icon solid major style1 fa-code"></span>
										<h3>Ipsum consequat</h3>
										<p>Sed lorem amet ipsum dolor et amet nullam consequat a feugiat consequat tempus veroeros sed consequat.</p>
									</li>
									<li>
										<span class="icon major style3 fa-copy"></span>
										<h3>Amed sed feugiat</h3>
										<p>Sed lorem amet ipsum dolor et amet nullam consequat a feugiat consequat tempus veroeros sed consequat.</p>
									</li>
									<li>
										<span class="icon major style5 fa-gem"></span>
										<h3>Dolor nullam</h3>
										<p>Sed lorem amet ipsum dolor et amet nullam consequat a feugiat consequat tempus veroeros sed consequat.</p>
									</li>
								</ul>
								<footer class="major">
									<ul class="actions special">
										<li><a href="generic.html" class="button">Learn More</a></li>
									</ul>
								</footer>
							</section> -->

						<!-- ImageFX Section -->
						<section id="imagefx" class="main special">
							<header class="major">
								<h2>ImageFX from Google</h2>
								<p>For nearly all occupations, ImageFX predominantly generated images of men who had lighter skin tones and most often appeared racially white, ethnically east Asian, south Asian, southeast Asian, or otherwise racially ambiguous. There were occupation-specific patterns of race and gender among the subjects of the images. ImageFX generated software developers, architects, biologists, and environmental scientists who mostly appeared white and civil engineers and statisticians who mostly appeared Asian. For nine of the ten occupations, ImageFX generated at most 10% of the subjects as women—with software developers, civil engineers, physicists, and geoscientists including zero women. The outlier for gender was nutritionists, 90% of whom ImageFX generated as women. In general, the images generated by ImageFX amplified stereotypes about people in STEM primarily being men who are white or Asian. 
								</p>
							</header>
							<footer class="major">
								<ul class="actions special">
									<li><a href="generic.html" class="button primary">See the ImageFX Images</a></li>
									<!-- <li><a href="generic.html" class="button">Learn More</a></li> -->
								</ul>
							</footer>
						</section>

						<!-- Stable Diffusion Section -->
							<section id="stablediffusion" class="main special">
								<header class="major">
									<h2>Stable Diffusion</h2>
									<!-- <p>Donec imperdiet consequat consequat. Suspendisse feugiat congue<br />
									posuere. Nulla massa urna, fermentum eget quam aliquet.</p> -->
								</header>
								<!-- <ul class="statistics">
									<li class="style1">
										<span class="icon solid fa-code-branch"></span>
										<strong>5,120</strong> Etiam
									</li>
									<li class="style2">
										<span class="icon fa-folder-open"></span>
										<strong>8,192</strong> Magna
									</li>
									<li class="style3">
										<span class="icon solid fa-signal"></span>
										<strong>2,048</strong> Tempus
									</li>
									<li class="style4">
										<span class="icon solid fa-laptop"></span>
										<strong>4,096</strong> Aliquam
									</li>
									<li class="style5">
										<span class="icon fa-gem"></span>
										<strong>1,024</strong> Nullam
									</li>
								</ul> -->
								<p class="content">The version of Stable Diffusion used was the Stable Diffusion XL 1.0 base model, without the associated refinement model. Similarly to ImageFX, Stable Diffusion predominantly generated images of men with lighter skin tones who appeared white or broadly ethnically Asian. There were occupation-specific patterns of race and gender among the subjects of the images. Stable Diffusion also did not generate any images of software developers, civil engineers, physicists, geoscientists, or chemists who were women. All of the nutritionists generated by Stable Diffusion were women. Stable Diffusion generated architects, biologists, and environmental scientists who mostly appeared white and civil engineers, statisticians, and software developers who mostly appeared Asian. Similarly to ImageFX, the images generated by Stable Diffusion amplified stereotypes about people in STEM primarily being white or Asian men. 
								</p>
								<footer class="major">
									<ul class="actions special">
										<li><a href="generic.html" class="button primary">See the Stable Diffusion Images</a></li>
									</ul>
								</footer>
							</section>

						<!-- Adobe Firefly Section -->
							<section id="adobefirefly" class="main special">
								<header class="major">
									<h2>Adobe Firefly</h2>
									<p>Firefly generally depicted women equitably for most occupations, with women representing 54% of all images—a significant jump compared to ImageFX and Stable Diffusion's respective rates of 14% and 22%. While most of the subjects in images generated by Firefly had lighter skin tones and/or appeared racially white, distributions of skin tone and perceived race appeared to be more random and less associated with specific occupations. Firefly also generated images of Black subjects, who are mostly absent from the image sets of ImageFX and Stable Diffusion. Firefly's results suggest that its model either is trained on substantially different data than ImageFX and Stable Diffusion, does not give significant weight to data that might correlate occupations with race and gender, or makes a conscious effort to generate images of people at rates more proportional to the actual demographics of the United States. Diverging from ImageFX and Stable Diffusion, Adobe Firefly did <i>not</i> amplify gendered and racial stereotypes about people in STEM. 
									</p>
								</header>
								<footer class="major">
									<ul class="actions special">
										<li><a href="generic.html" class="button primary">See the Adobe Firefly Images</a></li>
										<!-- <li><a href="generic.html" class="button">Learn More</a></li> -->
									</ul>
								</footer>
							</section>

					</div>

				<!-- Footer -->
					<footer id="footer">
						<section>
							<h2>Affiliations</h2>
							<p>This research was conducted by Seth Orvin within the Schedler Honors College at the University of Central Arkansas for presentation through the Justice and Equity Honors Network.</p>
						</section>
						<section>
							<h2>Contact Me</h2>
							<dl class="alt">
								<!-- <dt>Address</dt>
								<dd>1234 Somewhere Road &bull; Nashville, TN 00000 &bull; USA</dd>
								<dt>Phone</dt>
								<dd>(000) 000-0000 x 0000</dd> -->
								<dt>Email</dt>
								<dd>sorvin@cub.uca.edu</dd>
							</dl>
							<ul class="icons">
								<li><a href="https://github.com/sethorvin/GenAI-STEM-Stereotypes" class="icon brands fa-github alt"><span class="label">GitHub</span></a></li>
								<li><a href="https://www.linkedin.com/in/sethorvin/" class="icon brands fa-linkedin alt"><span class="label">LinkedIn</span></a></li>
								<li><a href="https://www.instagram.com/sethorvin/" class="icon brands fa-instagram alt"><span class="label">Instagram</span></a></li>
							</ul>
						</section>
						<p class="copyright">&copy; 2024 Seth Orvin. Design by <a href="https://html5up.net/stellar">HTML5 UP</a>.</p>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>